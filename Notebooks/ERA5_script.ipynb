{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi                       # Version: 0.7.5\n",
    "import xarray as xr                 # Version: 2023.12.0\n",
    "import geopandas as gpd             # Version: 1.4.0\n",
    "import numpy as np                  # Version: 1.24.3\n",
    "import pandas as pd                 # Version: 1.5.3\n",
    "from shapely.geometry import shape  # Version: 2.0.4\n",
    "import fiona                        # Version: 1.16.0\n",
    "import regionmask                   # Version: 0.13.0\n",
    "\n",
    "# Python version: 3.11.4\n",
    "import os\n",
    "import zipfile\n",
    "from glob import glob\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir   = os.path.dirname(notebook_dir)\n",
    "target_dir   = os.path.join(parent_dir, \"Data\", \"ERA5\")\n",
    "\n",
    "years = range(2005, 2021)\n",
    "months = range(1, 13)\n",
    "\n",
    "# cdsapirc_content = \"\"\"url: https://cds.climate.copernicus.eu/api\n",
    "# key: <INSERT KEY HERE>\n",
    "# \"\"\"\n",
    "\n",
    "# home = os.path.expanduser(\"~\")\n",
    "# file_path = os.path.join(home, \".cdsapirc\")\n",
    "\n",
    "# try:\n",
    "#     with open(file_path, \"w\") as f:\n",
    "#         f.write(cdsapirc_content)\n",
    "#     print(f\".cdsapirc created at: {file_path}\")\n",
    "# except PermissionError as e:\n",
    "#     print(f\"Permission denied. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cdsapi.Client()\n",
    "\n",
    "dataset = \"reanalysis-era5-single-levels\"\n",
    "\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        month_str = f\"{month:02d}\"\n",
    "        filename  = f\"era5_{year}_{month_str}.nc\"\n",
    "        target_file = os.path.join(target_dir, filename)\n",
    "        \n",
    "        if os.path.exists(target_file):\n",
    "            print(f\"Skipping {target_file} (already downloaded)\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Requesting data for {year}-{month_str}...\")\n",
    "        \n",
    "        request = {\n",
    "            \"product_type\": [\"reanalysis\"],\n",
    "            \"variable\": [\n",
    "                \"2m_dewpoint_temperature\",\n",
    "                \"2m_temperature\",\n",
    "                \"total_precipitation\",\n",
    "                \"10m_u_component_of_wind\",\n",
    "                \"10m_v_component_of_wind\",\n",
    "                \"surface_solar_radiation_downwards\"\n",
    "            ],\n",
    "            \"year\": [str(year)],\n",
    "            \"month\": [month_str],\n",
    "            \"day\": [f\"{d:02d}\" for d in range(1, 32)],\n",
    "            \"time\": [f\"{h:02d}:00\" for h in range(24)],\n",
    "            \"data_format\": \"netcdf\",\n",
    "            # \"download_format\": \"ZIP\",\n",
    "            \"area\": [-10, 112, -45, 155] \n",
    "        }\n",
    "\n",
    "        try:\n",
    "            client.retrieve(dataset, request).download(target_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {year}-{month_str}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXTRACT ZIP ###\n",
    "\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        month_str = f\"{month:02d}\"\n",
    "        filename  = f\"era5_{year}_{month_str}.nc\"\n",
    "        zip_path  = os.path.join(target_dir, filename)\n",
    "        real_zip_path = zip_path.replace(\".nc\", \".zip\")\n",
    "        unzip_path    = zip_path.replace(\".nc\", \"\")\n",
    "\n",
    "        if not os.path.exists(zip_path):\n",
    "            continue\n",
    "\n",
    "        os.rename(zip_path, real_zip_path)\n",
    "\n",
    "        with zipfile.ZipFile(real_zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(unzip_path)\n",
    "\n",
    "        os.remove(real_zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MERGE FILES ###\n",
    "\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        month_str = f\"{month:02d}\"\n",
    "        filename  = f\"era5_{year}_{month_str}\"\n",
    "        instant_path = os.path.join(target_dir, filename, \"data_stream-oper_stepType-instant.nc\")\n",
    "        accum_path   = os.path.join(target_dir, filename, \"data_stream-oper_stepType-accum.nc\")\n",
    "        output_file  = f\"era5_{year}_{month_str}.nc\"\n",
    "        output_path  = os.path.join(target_dir, output_file)\n",
    "\n",
    "        if not os.path.exists(instant_path):\n",
    "            print(f\"Missing instant: {instant_path}\")\n",
    "            continue\n",
    "        if not os.path.exists(accum_path):\n",
    "            print(f\"Missing accum: {accum_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ds_instant = xr.open_dataset(instant_path)\n",
    "            ds_accum   = xr.open_dataset(accum_path)\n",
    "            ds_merged  = xr.merge([ds_instant, ds_accum])\n",
    "            ds_merged.to_netcdf(output_path)\n",
    "            print(f\"Merged: {output_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed for {year}-{month_str}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREP ###\n",
    "\n",
    "geojson_file = os.path.join(parent_dir, \"Data\", \"states.geojson\")\n",
    "output_dir   = os.path.join(target_dir, \"CSV\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load and prepare regions\n",
    "with fiona.open(geojson_file) as src:\n",
    "    features = [f for f in src if f[\"properties\"][\"STATE_NAME\"] != \"Northern Territory\"]\n",
    "\n",
    "regions = [{\"name\": f[\"properties\"][\"STATE_NAME\"], \"geometry\": shape(f[\"geometry\"])} for f in features]\n",
    "gdf = gpd.GeoDataFrame(regions, crs=\"EPSG:4326\")\n",
    "region_mask = regionmask.Regions(name=\"states\", names=gdf[\"name\"], outlines=gdf[\"geometry\"])\n",
    "\n",
    "# Process each file\n",
    "all_results = []\n",
    "\n",
    "for nc_file in sorted(glob(os.path.join(target_dir, \"era5_*.nc\"))):\n",
    "    print(f\"Processing: {nc_file}\")\n",
    "    ds = xr.open_dataset(nc_file)\n",
    "    ds = ds.rename({'valid_time': 'time'})\n",
    "\n",
    "    ds['t2m'] = ds['t2m'] - 273.15\n",
    "    ds['d2m'] = ds['d2m'] - 273.15\n",
    "\n",
    "    def es(T): return np.exp((17.625 * T) / (243.04 + T))\n",
    "    ds['rh'] = 100 * es(ds['d2m']) / es(ds['t2m'])\n",
    "    ds['wind_speed'] = np.sqrt(ds['u10']**2 + ds['v10']**2)\n",
    "    ds['ssrd'] = ds['ssrd']  # Already in J/m²\n",
    "\n",
    "    # Create region mask\n",
    "    mask = region_mask.mask_3D(ds['longitude'], ds['latitude'])\n",
    "\n",
    "    # Daily resampling\n",
    "    # Variables that are summed over the day\n",
    "    sum_vars = ['tp', 'ssrd']\n",
    "    daily_sum = ds[sum_vars].resample(time='1D').sum()\n",
    "    daily_sum['tp'] = daily_sum['tp'] * 1000  # Convert from meters to millimeters\n",
    "    # Variables that are averaged\n",
    "    mean_vars = ['t2m', 'rh', 'wind_speed']\n",
    "    daily_mean = ds[mean_vars].resample(time='1D').mean()\n",
    "    daily_min  = ds[mean_vars + sum_vars].resample(time='1D').min()\n",
    "    daily_max  = ds[mean_vars + sum_vars].resample(time='1D').max()\n",
    "    daily_std  = ds[mean_vars + sum_vars].resample(time='1D').std()\n",
    "\n",
    "    # Regional aggregation\n",
    "    monthly_results = []\n",
    "\n",
    "    for idx, state_name in enumerate(gdf[\"name\"]):\n",
    "        region_data = {\n",
    "            \"State\": state_name,\n",
    "            \"Date\": daily_mean.time.values\n",
    "        }\n",
    "\n",
    "        for var in mean_vars:\n",
    "            da_mean = daily_mean[var].where(mask[idx]).mean(dim=[\"latitude\", \"longitude\"])\n",
    "            region_data[f\"{var}_mean\"] = da_mean.values.flatten()\n",
    "\n",
    "        for var in sum_vars:\n",
    "            da_sum = daily_sum[var].where(mask[idx]).mean(dim=[\"latitude\", \"longitude\"])\n",
    "            region_data[f\"{var}_sum\"] = da_sum.values.flatten()\n",
    "\n",
    "        for stat, dataset in zip([\"min\", \"max\", \"std\"], [daily_min, daily_max, daily_std]):\n",
    "            for var in mean_vars + sum_vars:\n",
    "                da = dataset[var].where(mask[idx]).mean(dim=[\"latitude\", \"longitude\"])\n",
    "                region_data[f\"{var}_{stat}\"] = da.values.flatten()\n",
    "\n",
    "        df = pd.DataFrame(region_data)\n",
    "        monthly_results.append(df)\n",
    "\n",
    "    final_df = pd.concat(monthly_results).reset_index(drop=True)\n",
    "\n",
    "    rename_map = {\n",
    "        \"tp\": \"Precipitation [mm/day]\",\n",
    "        \"rh\": \"Relative humidity [%]\",\n",
    "        \"ssrd\": \"Solar radiation [Jm2/day]\",\n",
    "        \"t2m\": \"Temperature [°C]\",\n",
    "        \"wind_speed\": \"Wind speed [m/s]\"\n",
    "    }\n",
    "\n",
    "    col_renamed = {}\n",
    "    for old_name, new_name in rename_map.items():\n",
    "        for suffix in [\"mean\", \"sum\", \"min\", \"max\", \"std\"]:\n",
    "            col = f\"{old_name}_{suffix}\"\n",
    "            if col in final_df.columns:\n",
    "                col_renamed[col] = f\"{new_name}_{suffix}\"\n",
    "\n",
    "    final_df = final_df.rename(columns=col_renamed)\n",
    "\n",
    "    all_results.append(final_df)\n",
    "\n",
    "# Merge to single file\n",
    "combined_df = pd.concat(all_results)\n",
    "combined_df.to_csv(os.path.join(output_dir, \"era5_stats_all.csv\"), index=False)\n",
    "print(\"Saved combined file: era5_stats_all.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
